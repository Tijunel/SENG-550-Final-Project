{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Machine Learning to Predict Amazon Reviews\n",
    "Data retrieved from: https://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install mlflow\n",
    "%pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"dbfs:/FileStore/shared_uploads/braden.thompson1@ucalgary.ca/Appliances_5-1.json\"\n",
    "df = spark.read.format(\"json\").load(\"\")\n",
    "#df = df.sample(0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ('asin', 'image', 'reviewTime', 'reviewerID', 'reviewerName', 'style', 'summary', 'unixReviewTime', 'verified', 'vote')\n",
    "df = df.drop(*cols_to_drop)\n",
    "df = df.dropna()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Ratings to Sentiment\n",
    "\n",
    "If a review is 4 or greater, it will recieve a score of 1. Otherwise, it will recieve a score of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def ratingToSentiment(rating):\n",
    "    if rating < 4:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "rating = udf(lambda x: ratingToSentiment(x))\n",
    "label = df.select('overall', 'reviewText', rating('overall'))\n",
    "sentimentLabels = label.withColumnRenamed('(overall)', 'sentiment')\n",
    "sentimentLabels.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplify Ratings\n",
    "\n",
    "This will change the ratings to lower case and remove the punctuation from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace, trim\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def simplifyRating(rating):\n",
    "    result = lower(rating)\n",
    "    return trim(regexp_replace(result,'\\p{Punct}',''))\n",
    "\n",
    "simplifyRatingDf = label.select(label.overall, simplifyRating(label.reviewText).alias('simplifiedText'), label.sentiment)\n",
    "simplifyRatingDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send Text to Vector\n",
    "\n",
    "This will send the text to a vector to allow it to more easily be viewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='simplifiedText', outputCol='vectorizedText')\n",
    "vectorizedDf = tokenizer.transform(simplifyRatingDf).select('overall', 'vectorizedText', 'sentiment')\n",
    "\n",
    "\n",
    "vectorizedDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol='vectorizedText', outputCol='vectorizedTextWithoutStopWords')\n",
    "vectorizedTextWithoutStopWordsDf = remover.transform(vectorizedDf).select('overall', 'vectorizedTextWithoutStopWords', 'sentiment')\n",
    "vectorizedTextWithoutStopWordsDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stemText(text):\n",
    "    result = []\n",
    "    for word in text:\n",
    "        stemmedText = stemmer.stem(word)\n",
    "        result.append(stemmedText)\n",
    "    return result\n",
    "\n",
    "stemmerUdf = udf(lambda x: stemText(x), ArrayType(StringType()))\n",
    "\n",
    "stemmedDf = (vectorizedTextWithoutStopWordsDf.withColumn('stemmedText', stemmerUdf('vectorizedTextWithoutStopWords')).select('overall', 'stemmedText', 'sentiment'))\n",
    "stemmedDf.show(10)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the IDF Model\n",
    "\n",
    "This will create the IDF and get the input ready to be read by rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "cv = CountVectorizer(inputCol='stemmedText', outputCol='countVectorized')\n",
    "cvModel = cv.fit(stemmedDf)\n",
    "cvTransformedDf = cvModel.transform(stemmedDf)\n",
    "\n",
    "idf = IDF()\n",
    "idf.setInputCol('countVectorized')\n",
    "idf.setOutputCol('idf')\n",
    "tfIdfModel = idf.fit(cv_transformed_df)\n",
    "tfIdfDf = tfIdfModel.transform(cv_transformed_df)\n",
    "tfIdfColumnDf = tfIdfDf.withColumn(\"sentiment\",col(\"sentiment\").cast(\"int\"))\n",
    "tfIdfColumnDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Rating Analysis & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tfIdfColumnDf.select('idf', 'overall').withColumnRenamed(\"idf\", \"features\").withColumnRenamed(\"overall\", \"label\")\n",
    "(train, test) = data.randomSplit([0.80, 0.20], seed=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up performance evaluators\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "accuracyEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "f1Evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "recallEvaluator = MulticlassClassificationEvaluator(metricName=\"weightedRecall\")\n",
    "precisionEvaluator = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\")\n",
    "\n",
    "accuracyResults = {}\n",
    "f1Results = {}\n",
    "recallResults = {}\n",
    "precisionResults = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up parameters\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, LinearSVC, NaiveBayes\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", regParam=1.0)\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "lrparamGrid = (ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.5, 2.0]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build())\n",
    "\n",
    "dtparamGrid = (ParamGridBuilder().addGrid(dt.maxDepth, [2, 5, 10, 20, 30]).addGrid(dt.maxBins, [10, 20, 40, 80, 100]).build())\n",
    "\n",
    "rfparamGrid = (ParamGridBuilder().addGrid(rf.maxDepth, [2, 5, 10, 20, 30]).addGrid(rf.maxBins, [10, 20, 40, 80, 100]).addGrid(rf.numTrees, [5, 20, 50, 100]).build())\n",
    "\n",
    "nbparamGrid = (ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up cross validation\n",
    "lrcv = TrainValidationSplit(estimator = lr, estimatorParamMaps = lrparamGrid, evaluator = accuracyEvaluator, trainRatio = 0.8)\n",
    "dtcv = TrainValidationSplit(estimator = dt,estimatorParamMaps = dtparamGrid, evaluator = accuracyEvaluator, trainRatio = 0.8)\n",
    "rfcv = TrainValidationSplit(estimator = rf, estimatorParamMaps = rfparamGrid, evaluator = accuracyEvaluator, trainRatio = 0.8)\n",
    "nbcv = TrainValidationSplit(estimator = nb, estimatorParamMaps = nbparamGrid, evaluator = accuracyEvaluator, trainRatio = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test the Model\n",
    "\n",
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "lrModel = lrcv.fit(train)\n",
    "lrPredictionDf = lrModel.transform(test)\n",
    "\n",
    "accuracyResults['Logistic Regression'] = accuracyEvaluator.evaluate(lrPredictionDf)\n",
    "f1Results['Logistic Regression'] = f1Evaluator.evaluate(lrPredictionDf)\n",
    "recallResults['Logistic Regression'] = recallEvaluator.evaluate(lrPredictionDf)\n",
    "precisionResults['Logistic Regression'] = precisionEvaluator.evaluate(lrPredictionDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtModel = dtcv.fit(train)\n",
    "dtPredictionDf = dtModel.transform(test)\n",
    "\n",
    "accuracyResults['Decision Tree'] = accuracyEvaluator.evaluate(dtPredictionDf)\n",
    "f1Results['Decision Tree'] = f1Evaluator.evaluate(dtPredictionDf)\n",
    "recallResults['Decision Tree'] = recallEvaluator.evaluate(dtPredictionDf)\n",
    "precisionResults['Decision Tree'] = precisionEvaluator.evaluate(dtPredictionDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = rfcv.fit(train)\n",
    "rfPredictionDf = rfModel.transform(test)\n",
    "\n",
    "accuracyResults['Random Forest'] = accuracyEvaluator.evaluate(rfPredictionDf)\n",
    "f1Results['Random Forest'] = f1Evaluator.evaluate(rfPredictionDf)\n",
    "recallResults['Random Forest'] = recallEvaluator.evaluate(rfPredictionDf)\n",
    "precisionResults['Random Forest'] = precisionEvaluator.evaluate(rfPredictionDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbModel = nbcv.fit(train)\n",
    "nbPredictionDf = nbModel.transform(test)\n",
    "\n",
    "accuracyResults['Naive Bayes'] = accuracyEvaluator.evaluate(nbPredictionDf)\n",
    "f1Results['Naive Bayes'] = f1Evaluator.evaluate(nbPredictionDf)\n",
    "recallResults['Naive Bayes'] = recallEvaluator.evaluate(nbPredictionDf)\n",
    "precisionResults['Naive Bayes'] = precisionEvaluator.evaluate(nbPredictionDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyResultsDf = sc.parallelize([ (k, v) for k,v in accuracyResults.items()]).toDF().withColumnRenamed(\"_1\", \"Model\").withColumnRenamed(\"_2\", \"Accuracy\")\n",
    "f1ResultsDf = sc.parallelize([ (k, v) for k,v in f1Results.items()]).toDF().withColumnRenamed(\"_1\", \"Model\").withColumnRenamed(\"_2\", \"F1\")\n",
    "recallResultsDf = sc.parallelize([ (k, v) for k,v in recallResults.items()]).toDF().withColumnRenamed(\"_1\", \"Model\").withColumnRenamed(\"_2\", \"Recall\")\n",
    "precisionResultsDf = sc.parallelize([(k, v) for k,v in precisionResults.items()]).toDF().withColumnRenamed(\"_1\",\"Model\").withColumnRenamed(\"_2\", \"Precision\")\n",
    "\n",
    "accuracyResultsDf.show(truncate=False)\n",
    "f1ResultsDf.show(truncate=False)\n",
    "recallResultsDf.show(truncate=False)\n",
    "precisionResultsDf.show(truncate=False)\n",
    "\n",
    "drawMetricsResults(AccuracyResultsDF, \\\n",
    "                   F1ResultsDF, \\\n",
    "                   RecallResultsDF, \\\n",
    "                   PrecisionResultsDF, \\\n",
    "                   (0, 1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
